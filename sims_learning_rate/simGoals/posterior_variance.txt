For all sims, and all runs, store the random seed.

Obtain the posterior variance via 20 points of the density, at time steps 
1,000; 2,000; ...; 10,000
For a single realization of the stationary chain
Redo 1 for 1,000 runs, so that at each of the time steps 1,000; 2,000; ...; 
10,000; we may plot a histogram of the variance.
Redo 3 for several values of the SNR.
Plot the posterior mean variance (with error bars) as a function of time. Put 1
curve per SNR value. So keep the number of SNR values around 6: something like 
0.1; 0.5; 1; 1.5; 2; 3.

Add one theoretical curve corresponding to the infinite SNR case.

This will already answer the question about speed of learning.
Redo 3 only for time step 1,000; but for a 1,000 distinct SNR values. Check 
that the average variance is a monotonic function (even injective) of the SNR.
Redo 3, only for time step 1,000; for many SNR values (around 1,000). Then, 
compute also P (Correct) across the 1,000 trials; for each SNR value. 
Also, for each simulated trajectory, also run the non-learning model from SIAM 
Rev and compute P (correct).

Then, plot P (Correct) as a function of the posterior variance, and mark the 
single point coming from the SIAM model.

If 4 showed injection of the mapping SNR -> mean variance, then the plots from 
5 should give us a notion of the impact of learning the rate on performance.
Compute the same curves as in 4, but instead of using the SNR to vary the 
variance, use the prior over h. That is, keep SNR and mode of prior fixed, and 
vary the prior variance across 1,000 distinct values.
For each run, use also SIAM Rev model.
We hope to get the same curves as in 4.
Redo 6, but for 6 distinct values of SNR. Superimpose curves on same plot.
